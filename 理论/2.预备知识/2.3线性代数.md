# 2.3线性代数
本节将介绍线性代数中的基本数学对象、算术和运算，并用数学符号和相应的代码实现来表示它们。
## 2.3.1标量
标量是一个单一的数，比如3.141592653589793，-1，0，100等。在Python中，标量通常用浮点数表示。

例如，北京的温度为52℉，如果要将此华氏度值转换为更常用的摄氏度， 则可以计算表达$c=\frac{5}{9}(f-32)$，在此等式中，每一项（`5`、`9`和`32`）都是标量值,符号`c`和`f`称为**变量**（variable），它们表示未知的标量值。

本教程采用了数学表示法，其中标量变量由普通小写字母表示（例如，`x`、`y`和`z`）。

用$\mathbb{R}$表示所有（连续）**实数标量的空间**，之后将严格定义**空间**（space）是什么。

现在只要记住表达式$x\in\mathbb{R}$是表示`x`是一个实值标量的正式形式

标量由只有一个元素的张量表示。 下面的代码将实例化两个标量，并执行一些熟悉的算术运算，即加法、乘法、除法和指数。
```python
import torch

x = torch.tensor(3.0)
y = torch.tensor(2.0)

x + y, x * y, x / y, x**y
```
输出：
```
(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))
```
## 2.3.2向量
向量可以被视为标量值组成的列表。 这些标量值被称为向量的元素（element）或分量（component）。 

 当向量表示数据集中的样本时，它们的值具有一定的现实意义。 例如，如果我们正在训练一个模型来预测贷款违约风险，可能会将每个申请人与一个向量相关联， 其分量与其收入、工作年限、过往违约次数和其他因素相对应。 

在数学表示法中，向量通常记为粗体、小写的符号 （例如， $\mathbf{x}$、$\mathbf{y}$和$\mathbf{z}$）。
通过一维张量表示向量。
```python
x = torch.arange(4)
x
```
输出：
```
tensor([0, 1, 2, 3])
```
我们可以使用下标来引用向量的任一元素，例如可以通过$x_i$
来引用第$i$个元素。 注意，元素$x_i$是一个标量，所以我们在引用它时不会加粗。 大量文献认为列向量是向量的默认方向，在本教程中也是如此。 

在数学中，向量$x$可以写为：
$$
\begin{split}\mathbf{x} =\begin{bmatrix}x_{1}  \\x_{2}  \\ \vdots  \\x_{n}\end{bmatrix},\end{split}
$$
其中$x_1,\ldots,x_n$是向量的元素。在代码中，我们通过张量的索引来访问任一元素。
```python
x[3]
```
输出：
```
tensor(3)
```
### 2.3.2.1长度、维度和形状
向量只是一个数字数组，就像每个数组都有一个长度一样，每个向量也是如此。 在数学表示法中，如果我们想说一个向量$x$由$n$个实值标量组成， 可以将其表示为$\mathbf{x}\in\mathbb{R}^n$。 向量的长度通常称为**向量的维度**（dimension）。

与普通的Python数组一样，我们可以通过调用Python的内置len()函数来访问张量的长度。
```python
len(x)
```
输出：
```
4
```
当用张量表示一个向量（只有一个轴）时，我们也可以通过`.shape`属性访问向量的长度。 形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。 对于只有一个轴的张量，形状只有一个元素。
```python
x.shape()
```
输出：
```
torch.Size([4])
```
请注意，维度（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。 为了清楚起见，我们在此明确一下： 

向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量。 然而，张量的维度用来表示张量具有的轴数。 在这个意义上，张量的某个轴的维数就是这个轴的长度。

## 2.3.3矩阵
正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。 矩阵，我们通常用粗体、大写字母来表示 （例如，$X$、$Y$和$Z$）， 在代码中表示为具有两个轴的张量。

数学表示法使用$\mathbf{A} \in \mathbb{R}^{m \times n}$来表示矩阵
，其由$m$行和$n$列的实值标量组成。 我们可以将任意矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$
视为一个表格， 其中每个元素$a_{ij}$属于第$i$行第$j$列：
$$
\begin{split}\mathbf{A}=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}.\end{split}
$$
当矩阵具有相同数量的行和列时，其形状将变为正方形； 因此，它被称为方阵（square matrix）。

当调用函数来实例化张量时， 我们可以通过指定两个分量$m$和$n$来创建一个形状为$m\times n$的矩阵。
```python
A = torch.arange(20).reshape(5, 4)
A
```
输出：
```
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15],
        [16, 17, 18, 19]])
```
我们可以通过行索引$i$和列索引$j$来访问矩阵中的标量元素$a_{ij}$， 例如$[\mathbf{A}]_{ij}$。

如果没有给出矩阵$A$的标量元素，可以简单地使用矩阵$A$的小写字母索引下标$a_{ij}$来引用$[\mathbf{A}]_{ij}$

当我们交换矩阵的行和列时，结果称为矩阵的转置（transpose）。通常用$\mathbf{a}^\top$来表示矩阵的转置，
如果$\mathbf{B}=\mathbf{A}^\top$，则对于任意$i$和$j$，都有$b_{ij}=a_{ji}$。

现在在代码中访问矩阵的转置。
```python
A.T
```
输出：
```
tensor([[ 0,  4,  8, 12, 16],
        [ 1,  5,  9, 13, 17],
        [ 2,  6, 10, 14, 18],
        [ 3,  7, 11, 15, 19]])
```

作为方阵的一种特殊类型，对称矩阵（symmetric matrix）$A$等于其转置：$\mathbf{A} = \mathbf{A}^\top$。

这里定义一个矩阵$B$：
```python
B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
B
```
输出：
```
tensor([[1, 2, 3],
        [2, 0, 4],
        [3, 4, 5]])
```
现在我们将B与它的转置进行比较。
```python
B == B.T
```
输出：
```
tensor([[ True,  True,  True],
        [ True,  True,  True],
        [ True,  True,  True]])
```

矩阵是有用的数据结构：它们允许我们组织具有不同模式的数据。 例如，我们矩阵中的行可能对应于不同的房屋（数据样本），而列可能对应于不同的属性。

## 2.3.4张量
就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构。

张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的$n$维数组的通用方法。 例如，向量是一阶张量，矩阵是二阶张量。

张量用特殊字体的大写字母表示（例如$X$，$Y$和$Z$，它的索引机制与矩阵类似。

当我们开始处理图像时，张量将变得更加重要，图像以$n$维数组形式出现， 其中3个轴对应于高度、宽度，以及一个通道（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）。

```python
X = torch.arange(24).reshape(2, 3, 4)
X
```
输出：
```
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],
 
        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
```

## 2.3.5张量算法的基本性质
标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。

- 从按元素操作的定义中可以注意到，任何按元素的一元运算都不会改变其操作数的形状。 

- 同样，给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。 例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。

```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
A, A + B
```
输出：
```
(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [12., 13., 14., 15.],
         [16., 17., 18., 19.]]),
 tensor([[ 0.,  2.,  4.,  6.],
         [ 8., 10., 12., 14.],
         [16., 18., 20., 22.],
         [24., 26., 28., 30.],
         [32., 34., 36., 38.]]))
```
具体而言，两个矩阵的按元素乘法称为**Hadamard积**（Hadamard product）（数学符号$\odot$）。

对于矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$和矩阵$\mathbf{B} \in \mathbb{R}^{m \times n}$，
矩阵$A$和$B$的Hadamard积为：
$$
\begin{split}\mathbf{A} \odot \mathbf{B} =
\begin{bmatrix}
    a_{11}  b_{11} & a_{12}  b_{12} & \dots  & a_{1n}  b_{1n} \\
    a_{21}  b_{21} & a_{22}  b_{22} & \dots  & a_{2n}  b_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \dots  & a_{mn}  b_{mn}
\end{bmatrix}.\end{split}
$$
```python
A * B
```
输出：
```
tensor([[  0.,   1.,   4.,   9.],
        [ 16.,  25.,  36.,  49.],
        [ 64.,  81., 100., 121.],
        [144., 169., 196., 225.],
        [256., 289., 324., 361.]])
```
将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。
```python
a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
```
输出：
```
(tensor([[[ 2,  3,  4,  5],
         [ 6,  7,  8,  9],
         [10, 11, 12, 13]],
 
        [[14, 15, 16, 17],
         [18, 19, 20, 21],
         [22, 23, 24, 25]]]),
torch.Size([2, 3, 4]))
```
## 2.3.6降维
### 2.3.6.1求和
我们可以对任意张量进行的一个有用的操作是计算其元素的和。数学表示法使用$\sum$符号表示求和。

为了表示长度为$d$的向量中元素的总和，可以记为$\sum_{i=1}^dx_i$。

在代码中可以调用计算求和的函数：
```python
x = torch.arange(4, dtype=torch.float32)
x, x.sum()
```
输出：
```
(tensor([0., 1., 2., 3.]), tensor(6.))
```
我们可以表示任意形状张量的元素和。 例如，矩阵$A$中元素的和可以记为$\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}$。
```python
A.shape,A.sum()
```
输出：
```
(torch.Size([5, 4]), tensor(190.))
```
默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定axis=0。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。
```python
A_sum_axis0 = A.sum(axis=0)
A_sum_axis0, A_sum_axis0.shape
```
输出：
```
(tensor([ 40.,  45.,  50.,  55.]), torch.Size([4]))
```
指定axis=1将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。
```python
A_sum_axis1 = A.sum(axis=1)
A_sum_axis1, A_sum_axis1.shape
```
输出：
```
(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))
```
沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。
```python
A.sum(axis=[0, 1])  # 结果和A.sum()相同
```
输出：
```
tensor(190.)
```

### 2.3.6.2平均值
一个与求和相关的量是**平均值**（mean或average）。 我们通过将总和除以元素总数来计算平均值。 在代码中，我们可以调用函数来计算任意形状张量的平均值。

```python
A.mean(), A.sum() / A.numel()
```
输出：
```
(tensor(9.5000), tensor(9.5000))
```
同样，计算平均值的函数也可以沿指定轴降低张量的维度。
```python
A.mean(axis=0), A.sum(axis=0) / A.shape[0]
```
输出：
```
(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))
```

### 2.3.6.3非降维求和
但是，有时在调用函数来计算总和或均值时保持轴数不变会很有用。
例如，在计算矩阵$A$中每行的平均值时，我们希望保持输出矩阵的行数与输入矩阵的行数相同。 这可以通过将keepdims设置为True来实现。
```python
sum_A = A.sum(axis=1, keepdims=True)
sum_A
```
输出：
```
tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
```
例如，由于sum_A在对每行进行求和后仍保持两个轴，我们可以通过广播将A除以sum_A。
```python
A / sum_A
```
输出：
```
tensor([[0.0000, 0.1667, 0.3333, 0.5000],
        [0.1818, 0.2273, 0.2727, 0.3182],
        [0.2105, 0.2368, 0.2632, 0.2895],
        [0.2222, 0.2407, 0.2593, 0.2778],
        [0.2286, 0.2429, 0.2571, 0.2714]])
```
如果我们想沿某个轴计算A元素的累积总和， 比如axis=0（按行计算），可以调用`cumsum`函数。 此函数不会沿任何轴降低输入张量的维度。
```python
A.cumsum(axis=0)
```
输出：
```
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  6.,  8., 10.],
        [12., 15., 18., 21.],
        [24., 28., 32., 36.],
        [40., 45., 50., 55.]])
```
## 2.3.7点积（Dot Product）
我们已经学习了按元素操作、求和及平均值。 另一个最基本的操作之一是点积。 给定两个向量$\mathbf{x},\mathbf{y}\in\mathbb{R}^d$，
它们的点积（dot product）$\mathbf{x}^\top\mathbf{y}$（或$\langle\mathbf{x},\mathbf{y}\rangle$）是**相同位置的按元素乘积的和**：
$\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i$。
```python
X = torch.arange(4, dtype=torch.float32)
y = torch.ones(4, dtype = torch.float32)
x, y, torch.dot(x, y)
```
输出：
```
(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))
```
注意，我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积：
```python
torch.sum(x * y)
```
输出：
```
tensor(6.)
```
点积在很多场合都很有用。 例如，给定一组由向量$\mathbf{x} \in \mathbb{R}^d$表示的值，和一组由$\mathbf{w} \in \mathbb{R}^d$表示的权重。
$x$中的值根据权重$w$的加权和，可以表示为点积$\mathbf{x}^\top \mathbf{w}$。当权重为非负数且和为1（即$\left(\sum_{i=1}^{d}{w_i}=1\right)$时， 点积表示**加权平均**（weighted average）。
将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。
后面的内容将正式介绍长度（length）的概念。

## 2.3.8矩阵-向量积

现在我们知道如何计算点积，可以开始理解**矩阵-向量积**（matrix-vector product）。

矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$和向量$\mathbf{x} \in \mathbb{R}^n$。让我们将矩阵$A$用它的行向量表示：
$$
\begin{split}\mathbf{A}=
\begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix},\end{split}
$$
其中每个$\mathbf{a}^\top_{i} \in \mathbb{R}^n$都是行向量，表示矩阵的第$i$行。

矩阵向量积$Ax$是一个长度为$m$的列向量， 其第$i$个元素是点积$\mathbf{a}^\top_i \mathbf{x}$：
$$
\begin{split}\mathbf{A}\mathbf{x}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix}\mathbf{x}
= \begin{bmatrix}
 \mathbf{a}^\top_{1} \mathbf{x}  \\
 \mathbf{a}^\top_{2} \mathbf{x} \\
\vdots\\
 \mathbf{a}^\top_{m} \mathbf{x}\\
\end{bmatrix}.\end{split}
$$
我们可以把一个矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$乘法看作一个从$\mathbb{R}^{n}$到$\mathbb{R}^{m}$向量的转换。

在代码中使用张量表示矩阵-向量积，我们使用mv函数。 当我们为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵-向量积。 注意，A的列维数（沿轴1的长度）必须与x的维数（其长度）相同。

```python
A.shape, x.shape, torch.mv(A, x)
```
输出：
```
(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))
```
## 2.3.9矩阵-矩阵乘法
矩阵-矩阵乘法（matrix-matrix multiplication）是深度学习库中最重要的操作之一。 给定两个矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$和$\mathbf{B} \in \mathbb{R}^{n \times p}$，它们的矩阵乘积$\mathbf{A}\mathbf{B}$是一个$m \times p$矩阵，其第$i$行第$j$列的元素是$\mathbf{A}$的第$i$行和$\mathbf{B}$的第$j$列的点积：
$$
(\mathbf{A}\mathbf{B})_{ij} = \mathbf{a}_{i\cdot}^\top \mathbf{b}_j = \sum_{k=1}^{n} a_{ik} b_{kj}.
$$











































































































































































